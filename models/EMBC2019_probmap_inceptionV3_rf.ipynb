{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMBC2019_probmap_inceptionV3_rf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wiahzM1EfnfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Ci2gPZb6ZIEw",
        "colab_type": "code",
        "outputId": "7ee9007d-8b75-4393-9f7e-542be1b1c083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Importing data via Google Drive Mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3N68kUFfu1I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading Train, Val, and Test Data Using Keras ImageDataGenerator"
      ]
    },
    {
      "metadata": {
        "id": "oeCnbRQ-ZVQO",
        "colab_type": "code",
        "outputId": "f26f5a1c-0595-430d-bf27-2dcf82acd647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "#Directory paths\n",
        "train_dir = \"/gdrive/My Drive/newCircleData/Train/\"\n",
        "val_dir = \"/gdrive/My Drive/newCircleData/Val/\"\n",
        "test_dir = \"/gdrive/My Drive/newCircleData/Test/\"\n",
        "\n",
        "#Image dims and training details\n",
        "img_width = 600\n",
        "img_height = 450\n",
        "batch_size = 1\n",
        "channels = 3\n",
        "epochs = 50\n",
        "nb_train_samples = 395\n",
        "nb_valid_samples = 145\n",
        "nb_test_samples = 197\n",
        "\n",
        "#Keras ImageDataGenerator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)    \n",
        "test_datagen = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, \n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True)   \n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True) #weight toward one class or another\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 395 images belonging to 2 classes.\n",
            "Found 145 images belonging to 2 classes.\n",
            "Found 197 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EjNpI6iLGxOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Loading Keras Pre-trained CNN & Extracting OCT Image Features**"
      ]
    },
    {
      "metadata": {
        "id": "-5V7skIXZQrr",
        "colab_type": "code",
        "outputId": "31e70436-ea8b-455e-f693-a2105c9c970b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Softmax, Flatten, Dense, BatchNormalization \n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "from keras import layers\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "#from keras.layers import Input, Dense\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.applications import ResNet50#, VGG16\n",
        "\n",
        "conv_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, channels), classes=2)\n",
        "\n",
        "#Feature dimensions for each model\n",
        "# conv_base = ResNet50(include_top=False, weights='imagenet',input_shape=(img_height, img_width, channels), pooling=None)\n",
        "\n",
        "# For VGG16\n",
        "# model_1st = 14\n",
        "# model_2nd = 18\n",
        "# model_3rd = 512\n",
        "\n",
        "# For ResNet50\n",
        "# model_1st = 15\n",
        "# model_2nd = 19\n",
        "# model_3rd = 2048\n",
        "\n",
        "# For ResNet18\n",
        "# model_1st = 15\n",
        "# model_2nd = 19\n",
        "# model_3rd = 512\n",
        "\n",
        "# For InceptionV3\n",
        "model_1st = 12\n",
        "model_2nd = 17\n",
        "model_3rd = 2048\n",
        "\n",
        "#Extracting features from OCT data using pretrained VGG\n",
        "def extract_features(dataset_type, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, model_1st, model_2nd, model_3rd))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    i = 0\n",
        "    if dataset_type == \"train\":\n",
        "        for inputs_batch, labels_batch in train_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break   \n",
        "    elif dataset_type == \"valid\":\n",
        "        for inputs_batch, labels_batch in valid_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    else:\n",
        "        for inputs_batch, labels_batch in test_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(\"train\", nb_train_samples)\n",
        "valid_features, valid_labels = extract_features(\"valid\", nb_valid_samples)\n",
        "test_features, test_labels = extract_features(\"test\", nb_test_samples)\n",
        "\n",
        "\n",
        "print(train_features.shape, train_labels.shape)\n",
        "print(valid_features.shape, valid_labels.shape)\n",
        "print(test_features.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 7s 0us/step\n",
            "(395, 12, 17, 2048) (395,)\n",
            "(145, 12, 17, 2048) (145,)\n",
            "(197, 12, 17, 2048) (197,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hdgxyp8mgBI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifier Layer: Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "pC1hJV8fsTGE",
        "colab_type": "code",
        "outputId": "f0c03bf6-b56f-4cc3-bcc8-8c29bb8910bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=12, random_state=0, bootstrap=True)\n",
        "clf.fit(np.reshape(train_features, (nb_train_samples, model_2nd*model_1st*model_3rd)), train_labels)\n",
        "\n",
        "prediction = clf.predict(np.reshape(valid_features, (nb_valid_samples, model_2nd*model_1st*model_3rd)))\n",
        "print(\"validation accuracy:\", sum([prediction[i] == valid_labels[i] for i in range(len(valid_labels))])/len(valid_labels))\n",
        "\n",
        "prediction = clf.predict(np.reshape(test_features, (nb_test_samples, model_2nd*model_1st*model_3rd)))\n",
        "print(\"test accuracy:\", sum([prediction[i] == test_labels[i] for i in range(len(test_labels))])/len(test_labels))\n",
        "\n",
        "print(test_labels)\n",
        "print(prediction)\n",
        "\n",
        "print('ROC AUC is', roc_auc_score(test_labels, prediction, 'weighted'))\n",
        "\n",
        "print([i for i in range(len(test_labels)) if prediction[i] != test_labels[i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation accuracy: 0.9379310344827586\n",
            "test accuracy: 0.934010152284264\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n",
            "ROC AUC is 0.9152368758002561\n",
            "[2, 10, 23, 27, 35, 43, 49, 55, 58, 67, 83, 96, 167]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5XaQCuhMgJnL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting False Positives and False Negatives"
      ]
    },
    {
      "metadata": {
        "id": "pQYd009Tkezi",
        "colab_type": "code",
        "outputId": "2272ab36-1e32-4b51-ac99-d475b8d3d39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# control_list, patient_list = test_generator.filenames[:141], test_generator.filenames[142:]\n",
        "\n",
        "# print(test_generator.filenames[:141])\n",
        "# print(test_generator.filenames[142:])\n",
        "\n",
        "# control_idx_list = [2, 7, 12, 33, 34, 40]\n",
        "# patient_idx_list = [0, 3, 7, 10, 16, 20, 35, 54]\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# patient_name_list = [patient_list[i] for i in patient_idx_list]\n",
        "# control_name_list = [control_list[i] for i in control_idx_list]\n",
        "\n",
        "# for i in range(len(patient_name_list)):\n",
        "#     files.download(test_dir+patient_name_list[i])\n",
        "\n",
        "# for i in range(len(control_name_list)):\n",
        "#     files.download(test_dir+control_name_list[i])\n",
        "\n",
        "filenames = test_generator.filenames   \n",
        "FP_list = []\n",
        "FN_list = []\n",
        "\n",
        "    #FP\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == 0 and prediction[i] == 1:\n",
        "    FP_list.append(filenames[i])\n",
        "\n",
        "#FN\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == 1 and prediction[i] == 0:\n",
        "    FN_list.append(filenames[i])\n",
        "    \n",
        "print(FP_list)\n",
        "print(FN_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['G/19220_RNFL_probability.png', 'G/20163_RNFL_probability.png', 'G/24215_RNFL_probability.png', 'G/25208_RNFL_probability.png', 'G/26768_RNFL_probability.png', 'G/28015_RNFL_probability.png', 'G/30631_RNFL_probability.png']\n",
            "['S/19088_RNFL_probability.png', 'S/19516_RNFL_probability.png', 'S/21395_RNFL_probability.png', 'S/27449_RNFL_probability.png', 'S/31801_RNFL_probability.png', 'S/View2168.png']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}