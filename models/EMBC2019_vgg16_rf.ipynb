{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMBC2019_vgg16_rf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hyd1559jt4x5",
        "colab_type": "code",
        "outputId": "2f9a80ba-014b-48fd-8b52-023d65830db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Importing Data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jIySELVzt8Lp",
        "colab_type": "code",
        "outputId": "ea0b7c46-c84b-457e-990d-99ec6f13a61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Directory paths\n",
        "train_dir = \"/gdrive/My Drive/newCircleData/Train/\"\n",
        "val_dir = \"/gdrive/My Drive/newCircleData/Val/\"\n",
        "test_dir = \"/gdrive/My Drive/newCircleData/Test/\"\n",
        "\n",
        "#Image dims and training details\n",
        "img_width = 600\n",
        "img_height = 450\n",
        "batch_size = 1\n",
        "channels = 3\n",
        "epochs = 50\n",
        "nb_train_samples = 395 #408\n",
        "nb_valid_samples = 145 #149\n",
        "nb_test_samples = 197 #192\n",
        "\n",
        "#Loading data using Keras ImageDataGenerator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)             \n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)    \n",
        "test_datagen = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, \n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True)   \n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True) #weight toward one class or another\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False)\n",
        "\n",
        "label_mapT = train_generator.class_indices\n",
        "print(label_mapT)\n",
        "\n",
        "label_mapV = valid_generator.class_indices\n",
        "print(label_mapV)\n",
        "\n",
        "label_mapTe = test_generator.class_indices\n",
        "print(label_mapTe)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 395 images belonging to 2 classes.\n",
            "Found 145 images belonging to 2 classes.\n",
            "Found 197 images belonging to 2 classes.\n",
            "{'G': 0, 'S': 1}\n",
            "{'G': 0, 'S': 1}\n",
            "{'G': 0, 'S': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ONTNCy7Qv18",
        "colab_type": "code",
        "outputId": "049a8c2e-c7ed-4f5c-b312-12794b84f4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Softmax, Flatten, Dense, BatchNormalization \n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "from keras import layers\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "from keras import layers\n",
        "from keras.applications import resnet50\n",
        "from keras import optimizers\n",
        "from keras.applications import VGG16\n",
        "\n",
        "#pretrained VGG16 on imagenet\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(img_height, img_width, channels))\n",
        "\n",
        "conv_base.summary()\n",
        "\n",
        "#Extracting features from OCT data using pretrained VGG\n",
        "def extract_features(dataset_type, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 14, 18, 512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    i = 0\n",
        "    if dataset_type == \"train\":\n",
        "        for inputs_batch, labels_batch in train_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break   \n",
        "    elif dataset_type == \"valid\":\n",
        "        for inputs_batch, labels_batch in valid_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    else:\n",
        "        for inputs_batch, labels_batch in test_generator:\n",
        "            features_batch = conv_base.predict(inputs_batch)\n",
        "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if i * batch_size >= sample_count:\n",
        "                break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(\"train\", nb_train_samples)\n",
        "valid_features, valid_labels = extract_features(\"valid\", nb_valid_samples)\n",
        "test_features, test_labels = extract_features(\"test\", nb_test_samples)\n",
        "\n",
        "\n",
        "print(train_features.shape, train_labels.shape)\n",
        "print(valid_features.shape, valid_labels.shape)\n",
        "print(test_features.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 6s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 450, 600, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 450, 600, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 450, 600, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 225, 300, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 225, 300, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 225, 300, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 112, 150, 128)     0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 112, 150, 256)     295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 112, 150, 256)     590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 112, 150, 256)     590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 56, 75, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 56, 75, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 56, 75, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 56, 75, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 28, 37, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 28, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 28, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 28, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 14, 18, 512)       0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(395, 14, 18, 512) (395,)\n",
            "(145, 14, 18, 512) (145,)\n",
            "(197, 14, 18, 512) (197,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRMsyZV3hthD",
        "colab_type": "code",
        "outputId": "6f62c94d-f7a7-44e0-80ac-036ebb53ad6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3488
        }
      },
      "cell_type": "code",
      "source": [
        "#Random Forest Training, Testing on validation and test sets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=12, random_state=0, bootstrap=True)\n",
        "trainResult = clf.fit(np.reshape(train_features, (395, 14*18*512)), train_labels)\n",
        "\n",
        "valid_prediction = clf.predict(np.reshape(valid_features, (145, 14*18*512)))\n",
        "print(\"validation accuracy:\", sum([valid_prediction[i] == valid_labels[i] for i in range(len(valid_labels))])/len(valid_labels))\n",
        "\n",
        "test_prediction = clf.predict(np.reshape(test_features, (197, 14*18*512)))\n",
        "print(\"test accuracy:\", sum([test_prediction[i] == test_labels[i] for i in range(len(test_labels))])/len(test_labels))\n",
        "\n",
        "trainResult = clf.fit(np.reshape(train_features, (395, 14*18*512)), train_labels)\n",
        "test_score = trainResult.predict_proba(np.reshape(test_features, (197, 14*18*512)))\n",
        "print(test_score)\n",
        "\n",
        "#Saving data for creating ROC curve\n",
        "import scipy.io as sio\n",
        "sio.savemat('/gdrive/My Drive/newCircleData/vgg_test_score.mat', {'vgg_test_score':test_score[:, ]})\n",
        "sio.savemat('/gdrive/My Drive/newCircleData/vgg_test_label.mat', {'vgg_test_label':test_labels[:, ]})\n",
        "\n",
        "print('ROC AUC is', roc_auc_score(test_labels, test_prediction, 'weighted'))\n",
        "\n",
        "print([i for i in range(len(test_labels)) if test_prediction[i] != test_labels[i]])\n",
        "\n",
        "#Getting false positives and false negatives\n",
        "filenames = test_generator.filenames   \n",
        "FP_list = []\n",
        "FN_list = []\n",
        "\n",
        "#FP\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == 0 and test_prediction[i] == 1:\n",
        "    FP_list.append(filenames[i])\n",
        "\n",
        "#FN\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == 1 and test_prediction[i] == 0:\n",
        "    FN_list.append(filenames[i])\n",
        "    \n",
        "print(FP_list)\n",
        "print(FN_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation accuracy: 0.9448275862068966\n",
            "test accuracy: 0.9390862944162437\n",
            "[[0.28 0.72]\n",
            " [0.67 0.33]\n",
            " [0.7  0.3 ]\n",
            " [0.69 0.31]\n",
            " [0.78 0.22]\n",
            " [0.85 0.15]\n",
            " [0.84 0.16]\n",
            " [0.78 0.22]\n",
            " [0.73 0.27]\n",
            " [0.85 0.15]\n",
            " [0.15 0.85]\n",
            " [0.83 0.17]\n",
            " [0.64 0.36]\n",
            " [0.86 0.14]\n",
            " [0.72 0.28]\n",
            " [0.82 0.18]\n",
            " [0.45 0.55]\n",
            " [0.8  0.2 ]\n",
            " [0.59 0.41]\n",
            " [0.79 0.21]\n",
            " [0.4  0.6 ]\n",
            " [0.88 0.12]\n",
            " [0.83 0.17]\n",
            " [0.46 0.54]\n",
            " [0.79 0.21]\n",
            " [0.89 0.11]\n",
            " [0.65 0.35]\n",
            " [0.54 0.46]\n",
            " [0.79 0.21]\n",
            " [0.83 0.17]\n",
            " [0.87 0.13]\n",
            " [0.86 0.14]\n",
            " [0.8  0.2 ]\n",
            " [0.73 0.27]\n",
            " [0.8  0.2 ]\n",
            " [0.28 0.72]\n",
            " [0.51 0.49]\n",
            " [0.71 0.29]\n",
            " [0.79 0.21]\n",
            " [0.76 0.24]\n",
            " [0.79 0.21]\n",
            " [0.81 0.19]\n",
            " [0.71 0.29]\n",
            " [0.69 0.31]\n",
            " [0.74 0.26]\n",
            " [0.8  0.2 ]\n",
            " [0.8  0.2 ]\n",
            " [0.8  0.2 ]\n",
            " [0.85 0.15]\n",
            " [0.78 0.22]\n",
            " [0.81 0.19]\n",
            " [0.89 0.11]\n",
            " [0.82 0.18]\n",
            " [0.87 0.13]\n",
            " [0.66 0.34]\n",
            " [0.79 0.21]\n",
            " [0.06 0.94]\n",
            " [0.24 0.76]\n",
            " [0.35 0.65]\n",
            " [0.   1.  ]\n",
            " [0.14 0.86]\n",
            " [0.54 0.46]\n",
            " [0.15 0.85]\n",
            " [0.3  0.7 ]\n",
            " [0.27 0.73]\n",
            " [0.   1.  ]\n",
            " [0.15 0.85]\n",
            " [0.55 0.45]\n",
            " [0.28 0.72]\n",
            " [0.07 0.93]\n",
            " [0.   1.  ]\n",
            " [0.07 0.93]\n",
            " [0.31 0.69]\n",
            " [0.   1.  ]\n",
            " [0.12 0.88]\n",
            " [0.06 0.94]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.06 0.94]\n",
            " [0.01 0.99]\n",
            " [0.35 0.65]\n",
            " [0.01 0.99]\n",
            " [0.16 0.84]\n",
            " [0.46 0.54]\n",
            " [0.07 0.93]\n",
            " [0.05 0.95]\n",
            " [0.15 0.85]\n",
            " [0.28 0.72]\n",
            " [0.24 0.76]\n",
            " [0.43 0.57]\n",
            " [0.7  0.3 ]\n",
            " [0.03 0.97]\n",
            " [0.14 0.86]\n",
            " [0.34 0.66]\n",
            " [0.24 0.76]\n",
            " [0.3  0.7 ]\n",
            " [0.73 0.27]\n",
            " [0.01 0.99]\n",
            " [0.12 0.88]\n",
            " [0.03 0.97]\n",
            " [0.07 0.93]\n",
            " [0.01 0.99]\n",
            " [0.01 0.99]\n",
            " [0.13 0.87]\n",
            " [0.08 0.92]\n",
            " [0.02 0.98]\n",
            " [0.23 0.77]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.09 0.91]\n",
            " [0.06 0.94]\n",
            " [0.   1.  ]\n",
            " [0.11 0.89]\n",
            " [0.08 0.92]\n",
            " [0.03 0.97]\n",
            " [0.03 0.97]\n",
            " [0.06 0.94]\n",
            " [0.09 0.91]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.08 0.92]\n",
            " [0.   1.  ]\n",
            " [0.19 0.81]\n",
            " [0.01 0.99]\n",
            " [0.01 0.99]\n",
            " [0.01 0.99]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.21 0.79]\n",
            " [0.13 0.87]\n",
            " [0.1  0.9 ]\n",
            " [0.07 0.93]\n",
            " [0.   1.  ]\n",
            " [0.08 0.92]\n",
            " [0.01 0.99]\n",
            " [0.03 0.97]\n",
            " [0.   1.  ]\n",
            " [0.09 0.91]\n",
            " [0.11 0.89]\n",
            " [0.01 0.99]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.11 0.89]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.19 0.81]\n",
            " [0.01 0.99]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.06 0.94]\n",
            " [0.02 0.98]\n",
            " [0.02 0.98]\n",
            " [0.   1.  ]\n",
            " [0.04 0.96]\n",
            " [0.   1.  ]\n",
            " [0.02 0.98]\n",
            " [0.36 0.64]\n",
            " [0.27 0.73]\n",
            " [0.04 0.96]\n",
            " [0.14 0.86]\n",
            " [0.5  0.5 ]\n",
            " [0.19 0.81]\n",
            " [0.07 0.93]\n",
            " [0.04 0.96]\n",
            " [0.29 0.71]\n",
            " [0.26 0.74]\n",
            " [0.15 0.85]\n",
            " [0.12 0.88]\n",
            " [0.   1.  ]\n",
            " [0.03 0.97]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.01 0.99]\n",
            " [0.02 0.98]\n",
            " [0.   1.  ]\n",
            " [0.01 0.99]\n",
            " [0.02 0.98]\n",
            " [0.03 0.97]\n",
            " [0.1  0.9 ]\n",
            " [0.05 0.95]\n",
            " [0.01 0.99]\n",
            " [0.02 0.98]\n",
            " [0.02 0.98]\n",
            " [0.04 0.96]\n",
            " [0.09 0.91]\n",
            " [0.   1.  ]\n",
            " [0.02 0.98]\n",
            " [0.29 0.71]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.04 0.96]\n",
            " [0.04 0.96]\n",
            " [0.12 0.88]\n",
            " [0.03 0.97]]\n",
            "ROC AUC is 0.9243277848911652\n",
            "[0, 10, 16, 20, 23, 35, 55, 61, 67, 90, 96, 162]\n",
            "['G/19160_RNFL_probability.png', 'G/20163_RNFL_probability.png', 'G/21757_RNFL_probability.png', 'G/23812_RNFL_probability.png', 'G/24215_RNFL_probability.png', 'G/26768_RNFL_probability.png']\n",
            "['S/19088_RNFL_probability.png', 'S/20057_RNFL_probability.png', 'S/21395_RNFL_probability.png', 'S/30455_RNFL_probability.png', 'S/31801_RNFL_probability.png', 'S/View2163.png']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}